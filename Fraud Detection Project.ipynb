{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34ba0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a2a9b",
   "metadata": {},
   "source": [
    "### Meta Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558363e0",
   "metadata": {},
   "source": [
    "In this project, we are predicting the probability that an online transaction is fraudulent, \n",
    "as denoted by the binary target isFraud. The data is broken into two files identity and transaction, \n",
    "which are joined by TransactionID. Not all transactions have corresponding identity information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070a26e",
   "metadata": {},
   "source": [
    "**Transaction Table**\n",
    "\n",
    "- **TransactionDT**: timedelta from a given reference datetime (not an actual timestamp)\n",
    "- **TransactionAmt**: transaction payment amount in USD\n",
    "- **ProductCD**: product code, the product for each transaction\n",
    "- **card1 - card6**: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "- **addr1, addr2**: “both addresses are for purchaser; addr1 as billing region; addr2 as billing country”\n",
    "- **dist: distance**; \"distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.”\n",
    "- **P_ and (R__) emaildomain**: purchaser and recipient email domain (certain transactions don't need recipient, so R_emaildomain is null.)\n",
    "- **C1-C14**: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked. (maybe counts of phone numbers, email addresses, names associated with the user, device, ipaddr, billingaddr, etc. Also these are for both purchaser and recipient, which doubles the number.)\n",
    "- **D1-D15**: timedelta, such as days between previous transaction, etc.\n",
    "- **M1-M9**: match, such as names on card and address, etc.\n",
    "- **Vxxx**: Vesta engineered rich features, including ranking, counting, and other entity relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ee82c",
   "metadata": {},
   "source": [
    "**Identity Table**\n",
    "\n",
    "- **DeviceType**\n",
    "- **DeviceInfo**\n",
    "- **id_01 - id_38**\n",
    "- Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n",
    "- They're collected by Vesta’s fraud protection system and digital security partners.\n",
    "- The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "75ac2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TransactionID - No missing\n",
    "#isFraud - No missing\n",
    "#TransactionDT - Time Stamp Data (Will clean later)\n",
    "#TransactionAmt - No missing\n",
    "#ProductCD - No missing\n",
    "#card1 - card6 - Card1: No missing, Card2: 8933, 1.5%, Card3: 1565, 0.26%, Card4: 1577, 0.27%, Card5: 4259, 0.72%, Card6: 1571, 0.27%\n",
    "#addr1 - addr2 - addr1: 65706, 11.12%, addr2: 65706, 11.12%\n",
    "#dist1 - dist2 - dist1: 352271, 59.65%, dist2: 552913, 93.62%\n",
    "#P_emaildomain, R_emaildomain - P_emaildomain: 94456, 15.99%, R_emaildomain: 453249, 76.75%\n",
    "#C1 - C14 - C1: No missing; C2: No missing; C3: No missing; C4: No missing; C5: No missing; C6: No missing; C7: No missing; C8: No missing; C9: No missing, C10: No missing, C11: No missing, C12: No missing, C13: No missing, C14: No missing;\n",
    "#D1 - D15\n",
    "#M1 - M9\n",
    "#V1 - V339"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b626d24",
   "metadata": {},
   "source": [
    "**Timeline**\n",
    "\n",
    "- 4/19: Transaction: Cards, addr, dist, email, \n",
    "- 4/21: Transaction: D, M\n",
    "- 4/23: Transaction: V\n",
    "- 4/25: Identity: Device Type, Device Info\n",
    "- 4/27: Identity: id_01 - id_38\n",
    "- 4/28: Done with Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f17cd8",
   "metadata": {},
   "source": [
    "- Check Column dependency\n",
    "    - Categorical Columns\n",
    "        - Group by missing or not, check final label distribution\n",
    "        - Group by missing or not, check other columns distribution\n",
    "        - Group by column labels, check other columns distribution\n",
    "\n",
    "- Filling Missing values for quantitative columns \n",
    "    - Compute the correlation between other quantitative columns\n",
    "    - Graph the scatterplot between the two columns\n",
    "    - Impute with value prediction based on other columns with large correlation (>=75%)\n",
    "    - Add a categorical column stating null or not\n",
    "\n",
    "- Filling Missing values for categorical columns \n",
    "    - Add null as a column value\n",
    "    - TBD\n",
    "    \n",
    "- One hot encode\n",
    "- Standadization/normalization\n",
    "    - Clip outlier values \n",
    "\n",
    "- PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9466841",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e17ee3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Training DataFrame: 590540 rows & 394 columns.\n",
      "Identity Training DataFrame: 144233 rows & 41 columns.\n",
      "Training DataFrame: 590540 rows & 434 columns.\n"
     ]
    }
   ],
   "source": [
    "#Data path\n",
    "train_transaction_path = \"data/train_transaction.csv\"\n",
    "train_identity_path = \"data/train_identity.csv\"\n",
    "test_transaction_path = \"data/test_transaction.csv\"\n",
    "test_identity_path = \"data/test_identity.csv\"\n",
    "sample_submission_path = \"data/sample_submission.csv\"\n",
    "\n",
    "#Read data\n",
    "train_transaction = pd.read_csv(train_transaction_path)\n",
    "train_identity = pd.read_csv(train_identity_path)\n",
    "test_transaction = pd.read_csv(test_transaction_path)\n",
    "test_identity = pd.read_csv(test_identity_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "train_df = pd.merge(train_identity, train_transaction, on=\"TransactionID\", how='outer')\n",
    "train_transaction_row = train_transaction.shape[0]\n",
    "train_transaction_col = train_transaction.shape[1]\n",
    "train_identity_row = train_identity.shape[0]\n",
    "train_identity_col = train_identity.shape[1]\n",
    "train_df_row = train_df.shape[0]\n",
    "train_df_col = train_df.shape[1]\n",
    "print('Transaction Training DataFrame: {} rows & {} columns.'.format(train_transaction_row, train_transaction_col))\n",
    "print('Identity Training DataFrame: {} rows & {} columns.'.format(train_identity_row, train_identity_col))\n",
    "print('Training DataFrame: {} rows & {} columns.'.format(train_df_row, train_df_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ca49c",
   "metadata": {},
   "source": [
    "### Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afca287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_col = train_identity.columns\n",
    "transaction_col = train_transaction.columns\n",
    "# train_df[train_df.columns[train_df.dtypes==object]].head(2)\n",
    "# train_transaction[train_transaction.columns[train_transaction.dtypes==object]].head(2)\n",
    "# train_identity[train_identity.columns[train_identity.dtypes==object]].head(2)\n",
    "cat_train_df = train_df[train_df.columns[train_df.dtypes==object]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5179d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a table with column name, unique value in each column, and the percentage of unique value in a column\n",
    "def df_summary_cat(df):\n",
    "    output = {'col_name': [], 'src': [], 'contain_nan': [], 'unique_val': [], \n",
    "              'unique_perc': [], 'null_val': [], 'null_perc': [], \n",
    "              'value': [], 'value_perc': [], 'most_common': [], 'redundent_perc': [], \n",
    "              'examples': []}\n",
    "\n",
    "    for i in df.columns:\n",
    "        unique_val = df[i].nunique()\n",
    "        if i in identity_col:\n",
    "            src = 'ID'\n",
    "        else:\n",
    "            src = 'TR'\n",
    "        unique_val_with_nan = len(df[i].unique())\n",
    "        contain_nan = True if unique_val != unique_val_with_nan else False\n",
    "        unique_perc = round(unique_val/df.shape[0] * 100, 3)\n",
    "        null_val = df[i].isnull().sum()\n",
    "        null_perc = round(null_val / df.shape[0] * 100, 2)\n",
    "        #The most common value / number of rows\n",
    "        if len(df[i].value_counts()) > 0:\n",
    "            redundent_perc = round(df[i].value_counts().iloc[0] / df.shape[0] * 100, 2)\n",
    "        else:\n",
    "            redundent_perc = 100\n",
    "        \n",
    "        value_val = df[i].describe()['count']\n",
    "        value_perc = round(value_val / df.shape[0] * 100, 2)\n",
    "        \n",
    "        examples_func = lambda x : list(df[i].value_counts().index) if (x <= 4) else list(df[i].value_counts().index[:4])\n",
    "        examples = examples_func(unique_val)\n",
    "        output['col_name'].append(i)\n",
    "        output['src'].append(src)\n",
    "        output['contain_nan'].append(contain_nan)\n",
    "        output['unique_val'].append(unique_val)\n",
    "        output['unique_perc'].append(str(unique_perc) + '%')\n",
    "        output['null_val'].append(null_val)\n",
    "        output['null_perc'].append(str(null_perc) + '%')\n",
    "        output['redundent_perc'].append(str(redundent_perc) + '%')\n",
    "        output['value'].append(value_val)\n",
    "        output['value_perc'].append(str(value_perc) + '%')\n",
    "        output['most_common'].append(df[i].value_counts().index[0])\n",
    "        output['examples'].append(examples)\n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac2eaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>src</th>\n",
       "      <th>contain_nan</th>\n",
       "      <th>unique_val</th>\n",
       "      <th>unique_perc</th>\n",
       "      <th>null_val</th>\n",
       "      <th>null_perc</th>\n",
       "      <th>value</th>\n",
       "      <th>value_perc</th>\n",
       "      <th>most_common</th>\n",
       "      <th>redundent_perc</th>\n",
       "      <th>examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_12</td>\n",
       "      <td>ID</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>446307</td>\n",
       "      <td>75.58%</td>\n",
       "      <td>144233</td>\n",
       "      <td>24.42%</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>20.83%</td>\n",
       "      <td>[NotFound, Found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_15</td>\n",
       "      <td>ID</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>449555</td>\n",
       "      <td>76.13%</td>\n",
       "      <td>140985</td>\n",
       "      <td>23.87%</td>\n",
       "      <td>Found</td>\n",
       "      <td>11.47%</td>\n",
       "      <td>[Found, New, Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_16</td>\n",
       "      <td>ID</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>461200</td>\n",
       "      <td>78.1%</td>\n",
       "      <td>129340</td>\n",
       "      <td>21.9%</td>\n",
       "      <td>Found</td>\n",
       "      <td>11.23%</td>\n",
       "      <td>[Found, NotFound]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_23</td>\n",
       "      <td>ID</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>585371</td>\n",
       "      <td>99.12%</td>\n",
       "      <td>5169</td>\n",
       "      <td>0.88%</td>\n",
       "      <td>IP_PROXY:TRANSPARENT</td>\n",
       "      <td>0.59%</td>\n",
       "      <td>[IP_PROXY:TRANSPARENT, IP_PROXY:ANONYMOUS, IP_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_27</td>\n",
       "      <td>ID</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>585371</td>\n",
       "      <td>99.12%</td>\n",
       "      <td>5169</td>\n",
       "      <td>0.88%</td>\n",
       "      <td>Found</td>\n",
       "      <td>0.87%</td>\n",
       "      <td>[Found, NotFound]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col_name src  contain_nan  unique_val unique_perc  null_val null_perc  \\\n",
       "0    id_12  ID         True           2        0.0%    446307    75.58%   \n",
       "1    id_15  ID         True           3      0.001%    449555    76.13%   \n",
       "2    id_16  ID         True           2        0.0%    461200     78.1%   \n",
       "3    id_23  ID         True           3      0.001%    585371    99.12%   \n",
       "4    id_27  ID         True           2        0.0%    585371    99.12%   \n",
       "\n",
       "    value value_perc           most_common redundent_perc  \\\n",
       "0  144233     24.42%              NotFound         20.83%   \n",
       "1  140985     23.87%                 Found         11.47%   \n",
       "2  129340      21.9%                 Found         11.23%   \n",
       "3    5169      0.88%  IP_PROXY:TRANSPARENT          0.59%   \n",
       "4    5169      0.88%                 Found          0.87%   \n",
       "\n",
       "                                            examples  \n",
       "0                                  [NotFound, Found]  \n",
       "1                              [Found, New, Unknown]  \n",
       "2                                  [Found, NotFound]  \n",
       "3  [IP_PROXY:TRANSPARENT, IP_PROXY:ANONYMOUS, IP_...  \n",
       "4                                  [Found, NotFound]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_summary_df = df_summary_cat(cat_train_df)\n",
    "cat_summary_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efabca",
   "metadata": {},
   "source": [
    "### Quantitative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b869a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_train_df = train_df[train_df.columns[train_df.dtypes!=object]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85b1283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_summary_quan(df):\n",
    "    output = {'col_name': [], 'src': [], 'contain_nan': [], 'unique_val': [], \n",
    "              'unique_perc': [], 'null_val': [], 'null_perc': [], 'redundent_perc': [],\n",
    "              'value': [], 'value_perc': [], 'mean': [], 'max': [], 'min': [], 'std': [], \n",
    "              '25%': [], '50%': [], '75%': [], 'examples': []}\n",
    "\n",
    "    for i in df.columns:\n",
    "        unique_val = df[i].nunique()\n",
    "        if i in identity_col:\n",
    "            src = 'ID'\n",
    "        else:\n",
    "            src = 'TR'\n",
    "        unique_val_with_nan = len(df[i].unique())\n",
    "        contain_nan = True if unique_val != unique_val_with_nan else False\n",
    "        unique_perc = round(unique_val/df.shape[0] * 100, 3)\n",
    "        null_val = df[i].isnull().sum()\n",
    "        null_perc = round(null_val / df.shape[0] * 100, 3)\n",
    "        if len(df[i].value_counts()) > 0:\n",
    "            redundent_perc = round(df[i].value_counts().iloc[0] / df.shape[0] * 100, 3)\n",
    "        else:\n",
    "            redundent_perc = 100\n",
    "        \n",
    "        value_val = df[i].describe()['count']\n",
    "        value_perc = round(value_val / df.shape[0] * 100, 2)\n",
    "        mean_val = df[i].describe()['mean']\n",
    "        std_val = df[i].describe()['std']\n",
    "        min_val = df[i].describe()['min']\n",
    "        twenty_five = df[i].describe()['25%']\n",
    "        fifty = df[i].describe()['50%']\n",
    "        seventy_five = df[i].describe()['75%']\n",
    "        max_val = df[i].describe()['max']\n",
    "        \n",
    "        examples_func = lambda x : list(df[i].value_counts().index) if (x <= 4) else list(df[i].value_counts().index[:4])\n",
    "        examples = examples_func(unique_val)\n",
    "        output['col_name'].append(i)\n",
    "        output['src'].append(src)\n",
    "        output['unique_val'].append(unique_val)\n",
    "        output['contain_nan'].append(contain_nan)\n",
    "        output['unique_perc'].append(unique_perc)\n",
    "        output['null_val'].append(null_val)\n",
    "        output['null_perc'].append(null_perc)\n",
    "        output['redundent_perc'].append(redundent_perc)\n",
    "        output['value'].append(value_val)\n",
    "        output['value_perc'].append(value_perc)\n",
    "        output['mean'].append(round(mean_val, 2))        \n",
    "        output['max'].append(max_val)        \n",
    "        output['min'].append(min_val)\n",
    "        output['std'].append(std_val)\n",
    "        output['25%'].append(twenty_five)\n",
    "        output['50%'].append(fifty)\n",
    "        output['75%'].append(seventy_five)\n",
    "        output['examples'].append(examples)\n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c81564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_summary_df = df_summary_quan(quant_train_df).sort_values('null_perc', ascending=False)\n",
    "quan_summary_df.head()\n",
    "quan_cols = ['null_perc', 'redundent_perc']\n",
    "# df_summary_quan(d_data).sort_values('null_perc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ba007",
   "metadata": {},
   "source": [
    "### Filter Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c09aba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #returns a table with column name, unique value in each column, and the percentage of unique value in a column\n",
    "# def df_summary(df):\n",
    "#     output = {'col_name': [], 'src': [], 'dtype' : [], 'unique_val': [], 'contain_nan': [],\n",
    "#               'unique_perc': [], 'null_val': [], 'null_perc': [], 'redundent_perc': [],\n",
    "#               'count': [], 'mean': [], 'max': [], 'min': [], 'std': [], \n",
    "#               '25%': [], '50%': [], '75%': [], 'examples': []}\n",
    "\n",
    "#     for i in df.columns:\n",
    "#         unique_val = df[i].nunique()\n",
    "#         if i in identity_col:\n",
    "#             src = 'ID'\n",
    "#         else:\n",
    "#             src = 'TR'\n",
    "#         unique_val_with_nan = len(df[i].unique())\n",
    "#         contain_nan = True if unique_val != unique_val_with_nan else False\n",
    "#         unique_perc = round(unique_val/df.shape[0] * 100, 3)\n",
    "#         null_val = df[i].isnull().sum()\n",
    "#         null_perc = round(null_val / df.shape[0] * 100, 3)\n",
    "#         if len(df[i].value_counts()) > 0:\n",
    "#             redundent_perc = round(df[i].value_counts().iloc[0] / df.shape[0] * 100, 3)\n",
    "#         else:\n",
    "#             redundent_perc = 100\n",
    "        \n",
    "#         if df[i].dtype == 'float64':\n",
    "#             count_val = df[i].describe()['count']\n",
    "#             mean_val = df[i].describe()['mean']\n",
    "#             std_val = df[i].describe()['std']\n",
    "#             min_val = df[i].describe()['min']\n",
    "#             twenty_five = df[i].describe()['25%']\n",
    "#             fifty = df[i].describe()['50%']\n",
    "#             seventy_five = df[i].describe()['75%']\n",
    "#             max_val = df[i].describe()['max']\n",
    "#         else:\n",
    "#             count_val = np.nan\n",
    "#             mean_val = np.nan\n",
    "#             std_val = np.nan\n",
    "#             min_val = np.nan\n",
    "#             twenty_five = np.nan\n",
    "#             fifty = np.nan\n",
    "#             seventy_five = np.nan\n",
    "#             max_val = np.nan\n",
    "        \n",
    "#         examples_func = lambda x : list(df[i].value_counts().index) if (x <= 4) else list(df[i].value_counts().index[:4])\n",
    "#         examples = examples_func(unique_val)\n",
    "#         output['col_name'].append(i)\n",
    "#         output['src'].append(src)\n",
    "#         output['dtype'].append(df[i].dtype)\n",
    "#         output['unique_val'].append(unique_val)\n",
    "#         output['contain_nan'].append(contain_nan)\n",
    "#         output['unique_perc'].append(unique_perc)\n",
    "#         output['null_val'].append(null_val)\n",
    "#         output['null_perc'].append(null_perc)\n",
    "#         output['redundent_perc'].append(redundent_perc)\n",
    "#         output['count'].append(count_val)\n",
    "#         output['mean'].append(mean_val)        \n",
    "#         output['max'].append(max_val)\n",
    "#         output['min'].append(min_val)\n",
    "#         output['std'].append(std_val)\n",
    "#         output['25%'].append(twenty_five)\n",
    "#         output['50%'].append(fifty)\n",
    "#         output['75%'].append(seventy_five)\n",
    "#         output['examples'].append(examples)\n",
    "#     return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "845fe2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df = df_summary(train_df)\n",
    "# summary_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8195d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Drop columns contains > 90% null observations\n",
    "# print(\"Null Value Percent > 90%: {} Columns\".format(summary_df[(summary_df['null_perc']>90)].shape[0]))\n",
    "# #Drop columns contains > 90% redundent information\n",
    "# print(\"Redundent Percent > 90%: {} Columns\".format(summary_df[(summary_df['redundent_perc']>90)].shape[0]))\n",
    "# #Drop columns contains > 90% null observations\n",
    "# #Drop columns contains > 90% redundent information\n",
    "\n",
    "# summary_df_clean = summary_df[(summary_df['null_perc']<=90) & (summary_df['redundent_perc']<=90)]\n",
    "# summary_df_clean.sort_values('null_perc', ascending=False)\n",
    "# summary_df[summary_df['redundent_perc']>=90] #redundent_perc > 90 && unique value < ???\n",
    "# summary_df[summary_df['unique_val']<2]\n",
    "# train_identity_box_df = train_identity.drop(columns=['TransactionID', 'id_02'])\n",
    "# train_identity.head(2)\n",
    "# train_transaction.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57f5e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction[['D1']].hist(bins=100)\n",
    "# uniform_data = np.random.rand(10, 12)\n",
    "\n",
    "\n",
    "\n",
    "# c_data = train_transaction[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']]\n",
    "# c_pear_corr=c_data.corr(method='pearson')\n",
    "# c_pear_corr\n",
    "\n",
    "# d_data = train_transaction[['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15']]\n",
    "# d_pear_corr=d_data.corr(method='pearson')\n",
    "# d_pear_corr\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8,8))\n",
    "# im = ax.imshow(c_pear_corr, interpolation='nearest')\n",
    "# fig.colorbar(im, orientation='vertical', fraction = 0.05)\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8,8))\n",
    "# im = ax.imshow(d_pear_corr, interpolation='nearest')\n",
    "# fig.colorbar(im, orientation='vertical', fraction = 0.05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16,16))\n",
    "# im = ax.imshow(pear_corr, interpolation='nearest')\n",
    "# fig.colorbar(im, orientation='vertical', fraction = 0.05)\n",
    "\n",
    "# # Show all ticks and label them with the dataframe column name\n",
    "# ax.set_xticklabels(our_data.columns, rotation=65, fontsize=20)\n",
    "# ax.set_yticklabels(our_data.columns, rotation=0, fontsize=20)\n",
    "\n",
    "# # Loop over data dimensions and create text annotations\n",
    "# for i in range(len(our_data.columns)-1):\n",
    "#     for j in range(len(our_data.columns)-1):\n",
    "#         text = ax.text(j, i, round(pear_corr.to_numpy()[i, j], 2),\n",
    "#                        ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "541f47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# cat_tr = cat_summary_df[cat_summary_df['src']=='TR'][['col_name', 'null_val', 'null_perc', 'examples']]\n",
    "# quan_tr = quan_summary_df[quan_summary_df['src']=='TR'][['col_name', 'null_val', 'null_perc', 'examples']]\n",
    "# tr_summary = pd.concat([cat_tr, quan_tr])\n",
    "# tr_summary.sort_values(by=['col_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7fb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3fd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1603b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cabc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcf2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ce601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63814854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394f866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a0974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1a855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31811c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e659d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9a26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75875cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3d30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80497d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63150a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71182c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a table with column name, unique value in each column, and the percentage of unique value in a column\n",
    "def df_summary_cat(df):\n",
    "    output = {'col_name': [], 'src': [], 'dtype' : [], 'unique_val': [], 'contain_nan': [],\n",
    "              'unique_perc': [], 'null_val': [], 'null_perc': [], 'redundent_perc': [],\n",
    "              'count': [], 'mean': [], 'max': [], 'min': [], 'std': [], \n",
    "              '25%': [], '50%': [], '75%': [], 'examples': []}\n",
    "\n",
    "    for i in df.columns:\n",
    "        unique_val = df[i].nunique()\n",
    "        if i in identity_col:\n",
    "            src = 'ID'\n",
    "        else:\n",
    "            src = 'TR'\n",
    "        unique_val_with_nan = len(df[i].unique())\n",
    "        contain_nan = True if unique_val != unique_val_with_nan else False\n",
    "        unique_perc = round(unique_val/df.shape[0] * 100, 3)\n",
    "        null_val = df[i].isnull().sum()\n",
    "        null_perc = round(null_val / df.shape[0] * 100, 3)\n",
    "        if len(df[i].value_counts()) > 0:\n",
    "            redundent_perc = round(df[i].value_counts().iloc[0] / df.shape[0] * 100, 3)\n",
    "        else:\n",
    "            redundent_perc = 100\n",
    "        \n",
    "        if df[i].dtype == 'float64':\n",
    "            count_val = df[i].describe()['count']\n",
    "            mean_val = df[i].describe()['mean']\n",
    "            std_val = df[i].describe()['std']\n",
    "            min_val = df[i].describe()['min']\n",
    "            twenty_five = df[i].describe()['25%']\n",
    "            fifty = df[i].describe()['50%']\n",
    "            seventy_five = df[i].describe()['75%']\n",
    "            max_val = df[i].describe()['max']\n",
    "        else:\n",
    "            count_val = np.nan\n",
    "            mean_val = np.nan\n",
    "            std_val = np.nan\n",
    "            min_val = np.nan\n",
    "            twenty_five = np.nan\n",
    "            fifty = np.nan\n",
    "            seventy_five = np.nan\n",
    "            max_val = np.nan\n",
    "        \n",
    "        examples_func = lambda x : list(df[i].value_counts().index) if (x <= 4) else list(df[i].value_counts().index[:4])\n",
    "        examples = examples_func(unique_val)\n",
    "        output['col_name'].append(i)\n",
    "        output['src'].append(src)\n",
    "        output['dtype'].append(df[i].dtype)\n",
    "        output['unique_val'].append(unique_val)\n",
    "        output['contain_nan'].append(contain_nan)\n",
    "        output['unique_perc'].append(unique_perc)\n",
    "        output['null_val'].append(null_val)\n",
    "        output['null_perc'].append(null_perc)\n",
    "        output['redundent_perc'].append(redundent_perc)\n",
    "        output['count'].append(count_val)\n",
    "        output['mean'].append(mean_val)        \n",
    "        output['max'].append(max_val)        \n",
    "        output['min'].append(min_val)\n",
    "        output['std'].append(std_val)\n",
    "        output['25%'].append(twenty_five)\n",
    "        output['50%'].append(fifty)\n",
    "        output['75%'].append(seventy_five)\n",
    "        output['examples'].append(examples)\n",
    "    return pd.DataFrame(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
